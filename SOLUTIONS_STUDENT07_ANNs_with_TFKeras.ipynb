{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_ANNs_with_TFKeras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "9XZEvoDUdSKh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Networks with TensorFlow.Keras\n",
        "\n",
        "In this notebook we will see how to use TensorFlow to train ANNs and avoid needing to extract features like we did in the previous notebook.\n",
        "\n",
        "Let's grab the same digit data we used before!"
      ]
    },
    {
      "metadata": {
        "id": "3MIrMPcldSQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b72d295d-e952-4dfd-982b-5b3dcafaaec1"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'free-spoken-digit-dataset' already exists and is not an empty directory.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qvpcS7sshZzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('free-spoken-digit-dataset/recordings')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6m3pXT_QW5Il",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "fcc8ee60-6cf7-48ae-aefd-e46723006d76"
      },
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "wavfiles = list()\n",
        "for file in glob.glob(\"*.wav\"):\n",
        "    wavfiles.append(file)\n",
        "\n",
        "num_points = len(wavfiles)\n",
        "\n",
        "print('{} data points\\n'.format(num_points))\n",
        "\n",
        "# list the first few files\n",
        "wavfiles[:10]\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500 data points\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4_jackson_31.wav',\n",
              " '8_theo_43.wav',\n",
              " '7_jackson_42.wav',\n",
              " '8_theo_5.wav',\n",
              " '2_nicolas_1.wav',\n",
              " '7_jackson_0.wav',\n",
              " '0_jackson_24.wav',\n",
              " '6_nicolas_15.wav',\n",
              " '9_nicolas_1.wav',\n",
              " '7_theo_45.wav']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "dY2O4r5bW-j7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile\n",
        "from IPython.display import Audio\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams.update({'font.size': 16,'figure.figsize':(10,6)})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbB7aYMZ2gMs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ]
    },
    {
      "metadata": {
        "id": "ADKsZNPW4L8J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "re_digit = re.compile('\\d+_')\n",
        "re_speaker = re.compile('_[a-z]+_')\n",
        "re_trial = re.compile('_\\d+.')\n",
        "\n",
        "X_data = np.ndarray(shape=(num_points,),dtype=object)\n",
        "y_labels = list()\n",
        "\n",
        "label='digit'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qcJGxFJ1fZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "69c99a5e-244b-48a9-abcf-6667baa08d0c"
      },
      "cell_type": "code",
      "source": [
        "ix = 0\n",
        "for sample_file in wavfiles:\n",
        "  try:\n",
        "    \n",
        "    digit = int(re.match( re_digit, sample_file)[0][:-1])\n",
        "    speaker = re.search( re_speaker, sample_file)[0][1:-1]\n",
        "\n",
        "    # read the file\n",
        "    (sample_rate, signal) = scipy.io.wavfile.read(sample_file)\n",
        "    \n",
        "    # some files have two channels... just take the first one\n",
        "    if len(signal.shape) == 2:\n",
        "      signal = signal[:,0]\n",
        "      \n",
        "    \n",
        "    X_data[ix,] = np.array(signal)\n",
        "    \n",
        "    ix+=1\n",
        "    \n",
        "    if label == 'speaker':\n",
        "      y_labels.append(speaker)\n",
        "      \n",
        "    elif label == 'digit':\n",
        "      y_labels.append(digit)\n",
        "      \n",
        "    else:\n",
        "      print(\"I don't know what you want to predict...\")\n",
        "      break\n",
        "      \n",
        "    \n",
        "  except Exception as e:\n",
        "    \n",
        "    # something went wrong!  =(\n",
        "    \n",
        "    print(sample_file)\n",
        "    print(e)\n",
        "    print(X_data.shape)\n",
        "    print(signal.shape)\n",
        "    \n",
        "    break\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  WavFileWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "bjprxl4__Mb8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalize the inputs\n",
        "Each .wav file is a different duration.  If we want to use the raw input as the signals, this will make our neural net cranky.  Let's artifically pad or truncate our input vectors so they're all the same length.  (Warning: we may lose data!)"
      ]
    },
    {
      "metadata": {
        "id": "rXTpE6du1fgv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove the mean from the data\n",
        "# For each data point:\n",
        "#   1) Remove the mean\n",
        "#   2) Make the length = 2000\n",
        "\n",
        "#s_plot_ix\n",
        "\n",
        "\n",
        "\n",
        "desired_length = 2000\n",
        "\n",
        "## YOUR CODE HERE\n",
        "\n",
        "num_waves = X_data.shape[0]\n",
        "\n",
        "for ix in range (num_waves):\n",
        "  len_sig= X_data[ix].shape[0]\n",
        "  \n",
        "  #its too short \n",
        "  if len_sig <desired_length:\n",
        "    pad_len = desired_length- len_sig\n",
        "    X_data[ix] = np.append(X_data[ix], np.mean(X_data[ix])*np.ones(pad_len))  #resetting some original data\n",
        "    \n",
        "  elif len_sig > desired_length:\n",
        "    X_data[ix] = X_data[ix][:desired_length]\n",
        "\n",
        "  #remove \n",
        "  X_data[ix] = X_data[ix]-np.mean(X_data[ix])\n",
        "  \n",
        "  \n",
        " # plt.subplot(2,1,2)\n",
        "  #plt.plot(X_data[s_plot_ix])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BwVVx3DaLybg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7cc0f686-f137-4589-ffab-9caa9049d301"
      },
      "cell_type": "code",
      "source": [
        "# turn y_labels into categories (0,1,2)\n",
        "u, labels = np.unique(y_labels, return_inverse=True)\n",
        "print(set(labels))\n",
        "## YOUR CODE HERE\n",
        "\n",
        "# Turn speaker names into categories [0,1,2]\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XZkW3thkFtPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split data into test/train split"
      ]
    },
    {
      "metadata": {
        "id": "mANSzEFx1fo7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split X and y into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, np.array(y_labels), test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1oGFU7vm1fmr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d72ded7d-c2e8-4b98-d763-8d69e257a533"
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1125,)\n",
            "(1125,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oNZex1ySHzGb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reformat the labels\n",
        "\n",
        "Next, we want to convert the labels from an integer format (e.g., \"2\"), to a [one hot encoding](https://en.wikipedia.org/wiki/One-hot) (e.g., \"0, 0, 1, 0, 0, 0, 0, 0, 0, 0\"). To do so, we'll use the `tf.keras.utils.to_categorical` [function](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function."
      ]
    },
    {
      "metadata": {
        "id": "WAVsA9rk1fkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "33eceab6-c744-495b-a489-a1c9b2d50ed3"
      },
      "cell_type": "code",
      "source": [
        "NUM_DIGITS = 10\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"Before\", y_train[0]) # The format of the labels before conversation\n",
        "y_train = tf.keras.utils.to_categorical(y_train, NUM_DIGITS)\n",
        "\n",
        "print(\"After\", y_train[0]) #after conversation\n",
        "y_train = tf.keras.utils.to_categorical(y_test, NUM_DIGITS)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "After [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DiaSczTAb042",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The X data is in a variable-length array, which will make TensorFlow cranky... we're going to hack something together to fix that\n",
        "\n",
        "\n",
        "X_train_df = pd.DataFrame()\n",
        "for xx in X_train:\n",
        "  X_train_df=X_train_df.append(pd.Series(xx),ignore_index=True)\n",
        "\n",
        "\n",
        "X_test_df = pd.DataFrame()\n",
        "for xx in X_test:\n",
        "  X_test_df=X_test_df.append(pd.Series(xx),ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dr9q1Sd0NdIO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Now, we'll create our neural network using the [Keras Sequential API](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential). \n",
        "* Architecture wise, we'll single layer network. \n",
        "* The hidden layer will have 512 units using the [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) activation function. \n",
        "* The output layer will have 10 units and use [softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) function. \n",
        "* Note: you will need to specify the input shape on the first layer. If you add subsequent layers, this is not necessary. \n",
        "* We will use the [categorical crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) loss function, and the [RMSProp](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop) optimizer."
      ]
    },
    {
      "metadata": {
        "id": "S9nggvb4HsYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "6b4150ca-a20b-4478-88d6-f5205c11cdcb"
      },
      "cell_type": "code",
      "source": [
        "# Build a fully-connected ANN with one hidden layer\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(X_train[0].shape)))  # hidden layer with 512 units and relu activation\n",
        "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))# output layer with 10 units and softmax activation  \n",
        "#normalized to sum up to one usually what you want the last layer to be in a categorical cell \n",
        "model.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               1024512   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,029,642\n",
            "Trainable params: 1,029,642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0eNUnAweNtnc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Next, we will train the model by using the [fit method](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) for 10 [epochs](https://www.quora.com/What-is-epochs-in-machine-learning). We will keep track of the training loss and accuracy as we go. Please be patient as this step may take a while depending on your hardware."
      ]
    },
    {
      "metadata": {
        "id": "W5YJMXeKHseP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "fbb65447-b908-497c-9950-9a3a741ffec6"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_df, y_train, epochs=10)    #you are going through the all of the data 10 times\n",
        "\n",
        "# plot metrics\n",
        "plt.plot(history.history['acc'])\n",
        "plt.show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-52c3b82f09f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#you are going through the all of the data 10 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# plot metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    919\u001b[0m       ]\n\u001b[1;32m    920\u001b[0m       \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m       \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    283\u001b[0m                      \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                      \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    286\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
            "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 1125 input samples and 375 target samples."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SxkMOdLrT-2V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "Now that we have trained our model, we want to evaluate it. The training accuracy wasn't great (~35%, depending on the random seed), but better than random (10%).  Let's see how well we do on our test data."
      ]
    },
    {
      "metadata": {
        "id": "86h3mYf5T_Bh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "8fff4163-6bcc-4aa6-a39d-3074cbeca1ef"
      },
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_df, y_test)\n",
        "print('Test accuracy: %.2f' % (accuracy))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8497c79f88fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy: %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    905\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    192\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (10,) but got array with shape (1,)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "J6d-b_SrHsl2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e35f2bfe-73b1-4f7e-c787-b1a59db967e5"
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict_proba(X_test_df, verbose=1)\n",
        "\n",
        "ix_pred = [np.argmax(y) for y in y_pred]\n",
        "ix_true = [np.argmax(y) for y in y_test]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "375/375 [==============================] - 0s 160us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I0iiseNLWyBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize the confusion matrix\n",
        "def plot_cmatrix(cm,labels,title='Confusion Matrix'):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  cax = ax.matshow(cm,cmap='Reds')\n",
        "  plt.title('\\n'+title+'\\n', fontsize=20)\n",
        "  fig.colorbar(cax)\n",
        "  ax.set_xticks(range(len(labels)))\n",
        "  ax.set_yticks(range(len(labels)))\n",
        "  ax.set_xticklabels(labels, fontsize=16)\n",
        "  ax.set_yticklabels(labels, fontsize=16)\n",
        "  plt.xlabel('Predicted', fontsize=16)\n",
        "  plt.ylabel('True', fontsize=16)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ViciIqK4Xz9t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "247bfe8a-836c-4adc-99b6-6e483b57d601"
      },
      "cell_type": "code",
      "source": [
        "cm = metrics.confusion_matrix(ix_true,ix_pred, range(10))\n",
        "plot_cmatrix(cm, range(10))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-c4e25478631c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mix_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_cmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-fc4acdb0f30d>\u001b[0m in \u001b[0;36mplot_cmatrix\u001b[0;34m(cm, labels, title)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAGrCAYAAABg2IjeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0167ace1d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "hN0uqYX6ZmH7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conclusions\n",
        "As you can see, this is actually a pretty terrible classifier... \n",
        "* Play around with some of the model parameters to see if you can make it better.  Later we'll see how we can use convolutions to solve a similar problem!\n",
        "* Try to see if you can train a better classifier using the frequency spectrum instead of the raw signal.  You won't have to pad/truncate the data because the data should all be the same size in the frequency domain (assuming they are all sampled at the same rate, which they are)!"
      ]
    },
    {
      "metadata": {
        "id": "Hm8YdwHuZwPj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}